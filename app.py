import gradio as gr
import torch
from PIL import Image
from transformers import Blip2Processor, Blip2ForConditionalGeneration, AutoTokenizer, AutoModelForCausalLM

# âœ… è®¾ç½®è®¾å¤‡ï¼ˆä½¿ç”¨ GPU åŠ é€Ÿï¼‰
device = "cuda" if torch.cuda.is_available() else "cpu"

# âœ… åŠ è½½ BLIP-2 è§†è§‰æ¨¡å‹
blip_processor = Blip2Processor.from_pretrained("Salesforce/blip2-flan-t5-xl")
blip_model = Blip2ForConditionalGeneration.from_pretrained("Salesforce/blip2-flan-t5-xl", torch_dtype=torch.float32).to(device)

# âœ… åŠ è½½ MedLLaMA 2 è¯­è¨€æ¨¡å‹
llama_tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-chat-hf")
llama_model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-2-7b-chat-hf", torch_dtype=torch.float32, device_map="auto")

# âœ… è§£æåŒ»å­¦å½±åƒ
def generate_image_description(image):
    image = image.convert("RGB")
    inputs = blip_processor(images=image, return_tensors="pt").to(device)
    generated_text = blip_model.generate(**inputs, max_new_tokens=50)
    description = blip_processor.tokenizer.decode(generated_text[0], skip_special_tokens=True)
    return description

# âœ… è®© MedLLaMA 2 è¿›è¡ŒåŒ»å­¦å½±åƒåˆ†æ
def analyze_medical_text(text_input):
    prompt = f"Patient's Medical Image Analysis:\n{text_input}\n\nProvide a detailed medical diagnosis and possible recommendations."
    inputs = llama_tokenizer(prompt, return_tensors="pt").to(device)
    output = llama_model.generate(**inputs, max_new_tokens=200)
    diagnosis = llama_tokenizer.decode(output[0], skip_special_tokens=True)
    return diagnosis

# âœ… å¤„ç†ç”¨æˆ·ä¸Šä¼ çš„ X-ray å›¾ç‰‡
def process_image(image):
    description = generate_image_description(image)  # å½±åƒæè¿°
    diagnosis = analyze_medical_text(description)  # å½±åƒè¯Šæ–­
    return description, diagnosis

# âœ… åˆ›å»º Gradio ç•Œé¢
with gr.Blocks() as demo:
    gr.Markdown("# ğŸ¥ AI åŒ»å­¦å½±åƒè¾…åŠ©è¯Šæ–­ (BLIP-2 + MedLLaMA 2)")
    
    with gr.Row():
        image_input = gr.Image(label="ä¸Šä¼ åŒ»å­¦å½±åƒï¼ˆå¦‚ X-rayï¼‰", type="pil")
        image_output = gr.Label(label="è‡ªåŠ¨ç”Ÿæˆçš„å½±åƒæè¿°")

    diagnosis_output = gr.Textbox(label="AI ç”Ÿæˆçš„åŒ»å­¦è¯Šæ–­", lines=5)
    
    analyze_button = gr.Button("âš¡ è¿›è¡Œ AI è¯Šæ–­")
    
    analyze_button.click(fn=process_image, inputs=image_input, outputs=[image_output, diagnosis_output])

# âœ… è¿è¡Œ Web åº”ç”¨
if __name__ == "__main__":
    demo.launch()
